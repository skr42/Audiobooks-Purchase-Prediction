{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c489401-6227-45dc-88e7-46d0fd64af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520ddd6b-6aaa-4cbc-a44e-89e867de5a76",
   "metadata": {},
   "source": [
    "# extract the dataset from audioApp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af9f08c2-f209-4df4-8444-677d5f2d465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_csv_data = np.loadtxt('Audiobooks_data.csv',delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a30de688-096b-468d-ab1e-f372fb04b5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.160e+03, 2.160e+03, 1.013e+01, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       [1.404e+03, 2.808e+03, 6.660e+00, ..., 0.000e+00, 0.000e+00,\n",
       "        1.820e+02],\n",
       "       [3.240e+02, 3.240e+02, 1.013e+01, ..., 0.000e+00, 1.000e+00,\n",
       "        3.340e+02],\n",
       "       ...,\n",
       "       [1.080e+03, 1.080e+03, 6.550e+00, ..., 0.000e+00, 0.000e+00,\n",
       "        2.900e+01],\n",
       "       [2.160e+03, 2.160e+03, 6.140e+00, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       [1.620e+03, 1.620e+03, 5.330e+00, ..., 0.000e+00, 0.000e+00,\n",
       "        9.000e+01]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unscaled_inputs_all = raw_csv_data[:,1:-1]\n",
    "unscaled_inputs_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bbd43df-d937-49a0-82e6-d36c33ed7132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_all = raw_csv_data[:,-1]\n",
    "targets_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdee810-00fc-4af6-81c2-c24adfe80129",
   "metadata": {},
   "source": [
    "## Now We Have to Balance the Dataset for equal Number of Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb6dff9d-94b3-4eb2-b67c-dd2157f7e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_one_targets = int(np.sum(targets_all))\n",
    "zero_targets_counter = 0\n",
    "indices_to_remove = []\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i] == 0:\n",
    "        zero_targets_counter += 1\n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)\n",
    "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)            \n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37b0dd4-2623-41f3-b494-fb8f2c939afc",
   "metadata": {},
   "source": [
    "# Standardize the inputs using preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "544f42b9-05eb-49c1-8f1e-7ebf7c29cd34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.18956512,  0.36398846,  0.67728889, ..., -0.8635056 ,\n",
       "        -0.20536617, -0.77240946],\n",
       "       [-0.33022754,  1.10843845, -0.08841391, ..., -0.8635056 ,\n",
       "        -0.20536617,  1.16499791],\n",
       "       [-2.50135991, -1.74528653,  0.67728889, ..., -0.8635056 ,\n",
       "         2.23179102,  2.78305242],\n",
       "       ...,\n",
       "       [ 1.18956512,  0.36398846,  0.67728889, ..., -0.20129479,\n",
       "        -0.20536617, -0.62337812],\n",
       "       [ 1.18956512,  0.36398846,  0.27347444, ..., -0.20129479,\n",
       "        -0.20536617,  0.21758442],\n",
       "       [ 1.18956512,  0.36398846,  0.20727535, ..., -0.20129479,\n",
       "        -0.20536617, -0.51692717]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)\n",
    "scaled_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f0961a-d5a7-4a0d-a7b3-96e090802bb4",
   "metadata": {},
   "source": [
    "## shuffle the data to be best as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1b4d130-e8bf-4686-a04d-24271976f9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 527,  985, 4399, ...,  950, 1934,  734])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "# Use the shuffled indices to shuffle the inputs and targets\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]\n",
    "shuffled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb4fa98-0c28-4ea6-9d77-c421f33d347b",
   "metadata": {},
   "source": [
    "## split the dataset into Train,validation and Test in the ratio of 8:1:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb1fa385-db84-4867-a546-843a781a1a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800.0 3579 0.5029337803855826\n",
      "208.0 447 0.465324384787472\n",
      "229.0 448 0.5111607142857143\n"
     ]
    }
   ],
   "source": [
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "# Count the samples in each subset, assuming we want 80-10-10 distribution of training, validation, and test.\n",
    "# Naturally, the numbers are integers.\n",
    "train_samples_count = int(0.8 * samples_count)\n",
    "validation_samples_count = int(0.1 * samples_count)\n",
    "\n",
    "# The 'test' dataset contains all remaining data.\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "\n",
    "# Create variables that record the inputs and targets for training\n",
    "# In our shuffled dataset, they are the first \"train_samples_count\" observations\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "# Create variables that record the inputs and targets for validation.\n",
    "# They are the next \"validation_samples_count\" observations, folllowing the \"train_samples_count\" we already assigned\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "# Create variables that record the inputs and targets for test.\n",
    "# They are everything that is remaining.\n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n",
    "\n",
    "# We balanced our dataset to be 50-50 (for targets 0 and 1), but the training, validation, and test were \n",
    "# taken from a shuffled dataset. Check if they are balanced, too. Note that each time you rerun this code, \n",
    "# you will get different values, as each time they are shuffled randomly.\n",
    "# Normally you preprocess ONCE, so you need not rerun this code once it is done.\n",
    "# If you rerun this whole sheet, the npzs will be overwritten with your newly preprocessed data.\n",
    "\n",
    "# Print the number of targets that are 1s, the total number of samples, and the proportion for training, validation, and test.\n",
    "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
    "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
    "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1e7936-5403-4359-9a80-80dc93dd5cb2",
   "metadata": {},
   "source": [
    "## Save the dataset for further used in .npz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a3c5aa9-d547-4bf9-899a-bc1ad271cad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Audiobooks_data_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('Audiobooks_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
    "np.savez('Audiobooks_data_test', inputs=test_inputs, targets=test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd02694-b68b-4e94-b119-956bb52486e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
